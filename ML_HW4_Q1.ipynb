{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdmiralJPJ/ai/blob/main/ML_HW4_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzOQoa2o1JpD"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FImT_Q-Z5tGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Resources used: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow book by Aurélien Géron, ChatGPT-4o, Google Gemini*"
      ],
      "metadata": {
        "id": "dOhxubu13DHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applied Machine Learning HW 3 Q1, Group 36, **Jack Tyndall**"
      ],
      "metadata": {
        "id": "__Pt6oY22MiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, Dataloader\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "class ClipImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, condition, categories, processor):\n",
        "        self.processor = processor\n",
        "        self.categories = categories\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        base_path = os.path.join(root_dir, condition)\n",
        "\n",
        "        for label_idx, category in enumerate(categories):\n",
        "            for filename in os.listdir(base_path):\n",
        "                if (filename.lower().endswith((\"jpg\", \"png\", \"jpeg\"))) and (category in filename.lower()):\n",
        "                    self.image_paths.append(os.path.join(base_path, filename))\n",
        "                    self.labels.append(label_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        processed = self.processor(images=image, return_tensors=\"pt\")\n",
        "        processed[\"pixel_values\"] = processed[\"pixel_values\"].squeeze(0)\n",
        "\n",
        "        return processed[\"pixel_values\"], label, path\n",
        "\n"
      ],
      "metadata": {
        "id": "_hfIVTuG7RzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "categories = [\"airplane\", \"car\", \"chair\", \"cup\", \"dog\", \"donkey\", \"duck\", \"hat\"]\n",
        "conditions = [\"realistic\", \"features\", \"geons\", \"silhouettes\", \"blurred\"]\n",
        "\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "real_dataset = ClipImageDataset(\n",
        "    root_dir=\"/content/drive/MyDrive/HW3/v0\",\n",
        "    condition=\"realistic\",\n",
        "    categories=categories,\n",
        "    processor=processor\n",
        ")\n",
        "\n",
        "feature_dataset = ClipImageDataset(\n",
        "    root_dir=\"/content/drive/MyDrive/HW3/v0\",\n",
        "    condition=\"features\",\n",
        "    categories=categories,\n",
        "    processor=processor\n",
        ")\n",
        "\n",
        "geon_dataset = ClipImageDataset(\n",
        "    root_dir=\"/content/drive/MyDrive/HW3/v0\",\n",
        "    condition=\"geons\",\n",
        "    categories=categories,\n",
        "    processor=processor\n",
        ")\n",
        "\n",
        "sil_dataset = ClipImageDataset(\n",
        "    root_dir=\"/content/drive/MyDrive/HW3/v0\",\n",
        "    condition=\"silhouettes\",\n",
        "    categories=categories,\n",
        "    processor=processor\n",
        ")\n",
        "\n",
        "blur_dataset = ClipImageDataset(\n",
        "    root_dir=\"/content/drive/MyDrive/HW3/v0\",\n",
        "    condition=\"blurred\",\n",
        "    categories=categories,\n",
        "    processor=processor\n",
        ")\n",
        "\n",
        "real_dataloader = DataLoader(real_dataset, batch_size=1, shuffle=False)\n",
        "feature_dataloader = DataLoader(feature_dataset, batch_size=1, shuffle=False)\n",
        "geon_dataloader = DataLoader(geon_dataset, batch_size=1, shuffle=False)\n",
        "sil_dataloader = DataLoader(sil_dataset, batch_size=1, shuffle=False)\n",
        "blur_dataloader = DataLoader(blur_dataset, batch_size=1, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Gn9hLrOFXaTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model configuration\n",
        "print(model.config)\n",
        "\n",
        "# Count total parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "# Vision encoder details\n",
        "print(\"Vision model:\", model.vision_model)\n",
        "print(\"Number of vision layers:\", model.config.vision_config.num_hidden_layers)\n",
        "print(\"Hidden size:\", model.config.vision_config.hidden_size)\n",
        "print(\"Number of attention heads:\", model.config.vision_config.num_attention_heads)\n",
        "\n",
        "# Text encoder details\n",
        "print(\"Text model:\", model.text_model)\n",
        "print(\"Number of text layers:\", model.config.text_config.num_hidden_layers)\n",
        "print(\"Text hidden size:\", model.config.text_config.hidden_size)\n",
        "\n",
        "# Break down parameters by component\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.shape} = {param.numel():,} parameters\")"
      ],
      "metadata": {
        "id": "EaSHdM2ImTIk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **HW4Q1 1.**\n",
        "\n",
        "I have chosen and downloaded the openai/clip-vit-base-patch32. This model uses a Vision Transformer architecture for its image encoder. It has 151,277,313 total parameters spread across two main components, a image encoder (vision model) and a text encoder (text model), both of which use transformer architecture. Both encoders have 12 transformer layers, each of which contain blocks and normalizations.\n",
        "\n",
        "For the image encoder/vision model, each transformer layer has 7,087,872 parameters, broken down like this:\n",
        "\n",
        "A Self Attention Block, with four projections (Query, Key, Value, and output projection) each at 590,592 parameters. Each projection has a matrix 768x768 and a bias of 768.\n",
        "\n",
        "Layer Normalization 1, 1,536 parameters, 768 weight vector and 768 bias.\n",
        "\n",
        "Multilayer Perceptron (MLP) Block with two dense fully connected (FC) layers, 2,362,368 (3072x768 weight + 3,072 bias) and 2,360,064 (768x3072 weight + 768 bias) parameters respectively.\n",
        "\n",
        "Layer Normalization 2, 1,536 parameters, 768 weight vector and 768 bias.\n",
        "\n",
        "For the text encoder/model, each layer has 3,152,384 parameters:\n",
        "\n",
        "The self attention block layout, with the four projections, this time at 262,656 parameters per projection with a 512x512 shape and 512 bias.\n",
        "\n",
        "Layer Normalization 1, 1,024 parameters, 512 vector + 512 bias.\n",
        "\n",
        "The MLP Block with two FCs, 1,050,624 (2048x512 weights + 2,048 bias) and 1,049,088 (512x2048 weights + 512 bias) respectively.\n",
        "\n",
        "Layer Normalization 2, 1,024 parameters, 512 vector + 512 bias.\n",
        "\n"
      ],
      "metadata": {
        "id": "6OT-OV9GZ66y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_inputs = processor(\n",
        "        text=categories,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "\n",
        "    text_features = model.get_text_features(**text_inputs)\n",
        "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "def evaluate_condition(dataloader, condition):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for pixel_values, label, _ in dataloader:\n",
        "            pixel_values = pixel_values.to(device)\n",
        "\n",
        "            image_features = model.get_image_features(pixel_values=pixel_values)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # Cosine similarity\n",
        "            similarity = image_features @ text_features.T\n",
        "            prediction = similarity.argmax(dim=-1).item()\n",
        "\n",
        "            y_true.append(label.item())\n",
        "            y_pred.append(prediction)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(categories))))\n",
        "\n",
        "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
        "\n",
        "    return cm, accuracy\n"
      ],
      "metadata": {
        "id": "ZVsPGLJuIoAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_cm, real_acc = evaluate_condition(real_dataloader, \"realistic\")\n",
        "\n",
        "print(\"\\nReal\")\n",
        "print(\"Accuracy:\", real_acc)\n",
        "print(\"Confusion Matrix:\", real_cm)\n",
        "\n",
        "feature_cm, feature_acc = evaluate_condition(feature_dataloader, \"features\"\n",
        "\n",
        "print(\"\\nFeatures\")\n",
        "print(\"Accuracy:\", feature_acc)\n",
        "print(\"Confusion Matrix:\", feature_cm)\n",
        "\n",
        "geon_cm, geon_acc = evaluate_condition(geon_dataloader, \"geons\")\n",
        "\n",
        "print(\"\\nGeons\")\n",
        "print(\"Accuracy:\", geon_acc)\n",
        "print(\"Confusion Matrix:\", geon_cm)\n",
        "\n",
        "sil_cm, sil_acc = evaluate_condition(sil_dataloader, \"silhouettes\")\n",
        "\n",
        "print(\"\\nSilhouettes\")\n",
        "print(\"Accuracy:\", sil_acc)\n",
        "print(\"Confusion Matrix:\", sil_cm)\n",
        "\n",
        "blur_cm, blur_acc = evaluate_condition(blur_dataloader, \"blurred\")\n",
        "\n",
        "print(\"\\nBlurred\")\n",
        "print(\"Accuracy:\", blur_acc)\n",
        "print(\"Confusion Matrix:\", blur_cm)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "su9VYRWWJ8jk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}