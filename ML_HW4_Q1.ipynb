{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzOQoa2o1JpD"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FImT_Q-Z5tGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Resources used: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow book by Aurélien Géron, ChatGPT-4o, Google Gemini*"
      ],
      "metadata": {
        "id": "dOhxubu13DHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applied Machine Learning HW 3 Q1, Group 36, **Jack Tyndall**"
      ],
      "metadata": {
        "id": "__Pt6oY22MiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import CLIPProcessor\n",
        "\n",
        "class ClipImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, condition, categories, processor):\n",
        "        self.processor = processor\n",
        "        self.categories = categories\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        base_path = os.path.join(root_dir, condition)\n",
        "\n",
        "        for label_idx, category in enumerate(categories):\n",
        "            for filename in os.listdir(base_path):\n",
        "                if (filename.lower().endswith((\"jpg\", \"png\", \"jpeg\"))) and (category in filename.lower()):\n",
        "                    self.image_paths.append(os.path.join(base_path, filename))\n",
        "                    self.labels.append(label_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        processed = self.processor(images=image, return_tensors=\"pt\")\n",
        "        processed[\"pixel_values\"] = processed[\"pixel_values\"].squeeze(0)\n",
        "\n",
        "        return processed[\"pixel_values\"], label, path\n",
        "\n"
      ],
      "metadata": {
        "id": "_hfIVTuG7RzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "categories = [\"airplane\", \"car\", \"chair\", \"cup\", \"dog\", \"donkey\", \"duck\", \"hat\"]\n",
        "\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "dataset = ClipImageDataset(\n",
        "    root_dir=\"/content/drive/MyDrive/HW3/v0\",\n",
        "    condition=\"realistic\",\n",
        "    categories=categories,\n",
        "    processor=processor\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "Gn9hLrOFXaTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **HW3Q1 1.**\n",
        "\n",
        "Images loaded in, gray scaled, and reduced to 128x128. Each condition is stored in a dict.\n",
        "\n",
        "Each number of components needed for 95% variance is displayed, and it tracks that the higher quality, more full images, require more components to maintain variance. Blurred images, which had little detail, required the least amount of components, while geons, which appear to be high definition simple 3D renderings, require the most.\n"
      ],
      "metadata": {
        "id": "6OT-OV9GZ66y"
      }
    }
  ]
}